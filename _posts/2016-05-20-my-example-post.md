---
layout: page
title:  "Ethics of Data Science in Cyber Security"
subtitle: ""
date:   2018-12-07 21:21:21 +0530
categories: ["Data Science"]
---

**Background Information**

<img src="{{ '/assets/img/0*zVYvHSJYCIreI2Ln.jpg' | prepend: site.baseurl }}" id="about-img" width="50%" height="50%">



<p>I am a Master’s in Data Science student at the University of Virginia and I am working on a project with the School of Engineering that involves detecting cyber-crime activity within the University network using a semi-supervised machine learning algorithm. The dataset for the project is a set of data points including personally identifiable information such as IP addresses of the users that interact with the University of Virginia network. The goal is to train a machine learning algorithm to detect the patterns that help in deciding whether an IP address is malicious or not. The final outcome of the project is a blacklist of the IP addresses which will be used by the cyber security team to prevent cyber-attacks in the future. An important sub-goal of the project is minimizing the instances of false positives where an IP address is diagnosed as malicious but is actually benign. While the project is intended to serve the common good of the university, it involves some serious ethical trade-offs and possibly some collateral damage. As the data scientist working on building this algorithm, it’s my responsibility to ensure that I address the ethical concerns around online privacy and data security.</p>

**Ethical Issues**

<p>Capturing personally identifiable information such as IP addresses is a necessity in pinpointing malicious activity with a substantial level of granularity. While this is important for the project as it will increase the model’s accuracy, there are risks involved in warehousing this information which could potentially invite more attacks or can be used for purposes that the user may or may not have consented to. When it comes to security of data, we do have guidelines from the Federal Trade Commission but are they good enough? Over the last few years, we have had massive data breaches in tech organizations like Equifax, Yahoo, eBay, Facebook etc., that invest a huge amount in money in putting together a state of the art cyber security infrastructure. Clearly, in the field of cyber security, no one or organization can do enough to protect themselves from an attack or a security breach. The bigger question is, is it worth storing personal information of individuals in an attempt to protect them from cyber-security threats when we do not have a complete understanding of possible threat?</p>

<p>One of the key aspects of the project involves characterizing IP addresses based on the patterns that machine learning algorithm detects before diagnosing them as malicious. Mathematically speaking, some IP addresses will be categorized as malicious based on a benchmarked statistic derived from a sample of the dataset. Based on this, a blacklist is created and deployed to the University of Virginia. There is a small chance that the sample (used for computing the benchmark statistic) failed to provide an accurate representation of the population and ended up denying access to users that were not engaging in malicious activity. Algorithms make decisions and resolve trade-offs in ways that are different from human and are often limited by the data that is fed to them. While the algorithm might be efficient at neutralizing cyber threats but in the blatant pursuit of accuracy of prediction, it can lose sight of the inherent discrimination it might be propagating and can be stuck in feedback loops. As a result of algorithmic bias or generalized and sometimes biased grouping of users by the algorithm, is there more potential harm than potential good?</p>

<p>For the purpose of building a database of possible malicious activity that could intrude the University’s network, we set up booby traps in the networks called honeypots to lure intruders or attackers and track their movement through the network. As soon as the the hackers are trapped in honeypot, everyone in the cybersecurity team at UVa is alerted and they could potentially stop them at their tracks. However, in order to get a complete understanding of how botnets or hackers operate, we let them perform operations which could involve stealing personal information. Tracking the hackers reveal invaluable insights into attacker’s techniques and network vulnerabilities and it also helps make the intrusion detection systems in the network better. The use of honeypots is a very controversial topic and although perfectly legal, how ethical is it to lure someone into stealing an object?</p>

<p>Like most Data Science problems, there is not enough information for us to predict malicious activity with complete certainty. An IP address (A section of the network) is considered the smallest unit of classification for the algorithm because that is level of resolution of information the server is able to capture with the current technical setup. A more granular unit of classification would be individual computers but unfortunately, we do not have this information. A smaller unit of classification could potentially reduce the number of incidents of benign users losing access to the internet. As a result, this is a minor design flaw in the methodology adopted for the project. The School of Engineering at UVa believes that in spite of minor flaws in the data, it is worthwhile building an algorithm to combat cyberthreats. While this does not solve any of the issues mentioned above, this information in crucial in formulating potential solutions for the ethical dilemmas highlighted in this paper.</p>

**Potential Solutions**

<p>Data security and privacy is an important part of the capstone project and it is given paramount importance by the School of Engineering at UVa. One of the methods in which cybersecurity team at UVa is ensuring security of personal information is by anonymizing the records so that it is not possible to connect the data to an actual person. The anonymization is performed by using another algorithm that scrambles the IP address (PII) so that it is decipherable only to the cybersecurity team that has the key to the decryption. Although anonymization might seem like an elegant solution to the issue but it does not provide a clear solution to the underlying problem</p>

<p>Helen Nissenbaum, in her paper “Big Data’s End Run around Anonymity and Consent” argues that anonymization of PII makes them non-identifiable but does not make them unreachable. A recent study found that students suffering from depression could be identified from their internet browsing patterns. In the context of this project, there is a deeper implication of Nissenbaum’s comments. The algorithm, in question, characterizes users based on their activity on the UVa network and not on the basis of their IP address. The IP address happens to be just an identifier and removing or anonymizing it is not going to affect the performance of the algorithm. In fact, the features that are used to characterize users remain in the database and the algorithm can certainly discriminate against a certain group without knowing any of their PII. If the algorithm that I am working on can potentially cause harm, there is no telling what cyber terrorists could if they get their hands on this information. The reason why anonymization was considered as a solution by the cyber security team is because most technological organizations treat data security and privacy as a legal compulsion rather than an ethical obligation. As per legal norms set by the governing body of cybersecurity, anonymization counts as meeting the required standards for data security of IP addresses. In addition to the anonymization, the School of Engineering should make it mandatory for all researchers that are interested in working with this data to go through a review from an Independent Ethics Committee or Research Ethics Board. This will ensure that the impact on data privacy and security is measured before commencing research on the data.</p>

<p>Algorithmic bias has been cited in cases ranging from election outcomes to the spread of online hate speech. Albeit this is an easier problem to comprehend, it is much harder problem to solve. In the field of Data Science, algorithmic bias is treated by ensuring that distribution between different groups of users or data is equitable and by constantly updating the algorithms with new information. On the surface, this may seem like a way out of algorithmic bias, but this works only in ideal scenarios where it is actually possible to have an equitable distribution in classes. In reality, there are only a handful of instances in the past where such techniques were successful to alleviating algorithmic bias. According to Sasha Eder, COO of NewtonX, there are three primary reasons why algorithmic bias is perpetuated. It is either due to diversity gaps in the dataset, personal preferences of the Data Analyst related to the definition of the outcome or due to the involvement of a sophisticated and uninterpretable algorithm. Diversity gaps and using a simpler algorithm can be solved by consulting an expert statistician but the inducing personal preferences (in the form of a benchmark statistic in the case of my capstone project) into the algorithm cannot be solved by a technologist. In my opinion, having an ethics committee of social scientists review such algorithms before they are deployed can help keep personal preferences at an arm’s length distance.</p>

<p>Honeypots remain a controversial topic and although they are generally seen as being a legal solution, not everyone sees them as ethical. Pelletier and Kabay, make the following statement, ‘As for entrapment of hackers using honeypot, although this is not a legal problem, this does not mean that the way a honeypot entices attackers is not unethical’. The stakeholders involved in this issue are researchers, malicious actors and the users who are a part of the network. Using honeypots benefits the researchers in studying malicious actors but could prove detrimental to the users. The primary argument against implementation of honeypots is that exposing network vulnerabilities in order to study a botnet specimen (attacker) is too much of a risk posed to the users belonging to the network. Little work has been done in this area to ensure that the damage inflicted due to the honeypot system is minimal or to find alternate approaches to study botnets. As researchers, while we are interested in learning about the techniques that a malicious actor might adopt, we need to respect those who we are trying to protect. We should communicate to the users our proposed research methods, help them understand the ramifications of implementing these methods and acquire their full consent. In addition to that, we should do our best as individuals to respect their autonomy and do everything in our power to minimize the harm they experience.</p>

**Conclusion**

<p>In my opinion, most ethical challenges faced in the field of technology and especially in the field of machine learning are due to the limited presence of social scientists. As the government agencies are unable to keep pace with the advancements in Silicon Valley, it falls on the tech organizations to ensure that they have an ethics review committee to perform risk assessment from an ethical perspective.
I have expert guidance from one of my professors for putting this together. If you are interested in working on such themes, I would highly recommend checking out his website https://www.samuellengen.net.</p>
